{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Applied AI with Deep Learning Course\n",
    "Taken November 2018 \n",
    "\n",
    "### The following will be how to use pytorch a rising deep learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 307\n",
      "-rw-r--r-- 1 My LENOVO 197121   5775 Nov 16 20:48 intro_pytorch.ipynb\n",
      "drwxr-xr-x 1 My LENOVO 197121      0 Nov  9 22:28 Deep_learning_2018\n",
      "-rw-r--r-- 1 My LENOVO 197121  45108 Jun 16 16:41 wk1ICL_mean_cov.ipynb\n",
      "-rw-r--r-- 1 My LENOVO 197121  20429 Jun  6 06:51 imperial_college_wk4.ipynb\n",
      "-rw-r--r-- 1 My LENOVO 197121  18113 Jun  6 06:41 imperial_college_wk2.ipynb\n",
      "-rw-r--r-- 1 My LENOVO 197121  18848 Jun  6 06:31 imperial_college_wk1.ipynb\n",
      "-rw-r--r-- 1 My LENOVO 197121 155062 Jun  4 21:06 multivariate_chain_rule.pdf\n",
      "drwxr-xr-x 1 My LENOVO 197121      0 Mar  3  2018 Old\n",
      "drwxr-xr-x 1 My LENOVO 197121      0 Jan 10  2018 w4 Neural Nets\n",
      "drwxr-xr-x 1 My LENOVO 197121      0 Dec  2  2017 Assignment\n",
      "-rw-r--r-- 1 My LENOVO 197121    162 Oct 23  2017 ~$M Exercise.docx\n",
      "-rw-r--r-- 1 My LENOVO 197121    162 Jul  3  2017 ~$tave commands.docx\n",
      "drwxr-xr-x 1 My LENOVO 197121      0 Jun  7  2017 Week 2\n",
      "-rw-r--r-- 1 My LENOVO 197121     45 May 27  2017 desktop.ini\n",
      "drwxr-xr-x 1 My LENOVO 197121      0 May 27  2017 Week 3 Log Reg\n"
     ]
    }
   ],
   "source": [
    "!ls -lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x56c4cb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "torch.manual_seed(123) #always good to seed before initialiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "#create a torch.tensor object from python list\n",
    "v = [1,2,3]\n",
    "print(type(v))\n",
    "v_tensor = torch.Tensor(v)\n",
    "print(v_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [2., 4., 9.]])\n"
     ]
    }
   ],
   "source": [
    "##create a torch.Tensor object of size 2x3 from 2x3 matrix\n",
    "m2x3 = [[1,2,3],[2,4,9]]\n",
    "m2x3_tensor = torch.Tensor(m2x3)\n",
    "print(m2x3_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  1.,   2.,   3.],\n",
      "         [  4.,   5.,   6.],\n",
      "         [  7.,   8.,   9.]],\n",
      "\n",
      "        [[  1.,   2.,   9.],\n",
      "         [ 16.,  25.,  36.],\n",
      "         [ 49.,  64.,  81.]],\n",
      "\n",
      "        [[  1.,   8.,  27.],\n",
      "         [ 64., 125., 216.],\n",
      "         [343., 512., 729.]]])\n"
     ]
    }
   ],
   "source": [
    "##create a 3-d torch.Tensor object of size 3x3x3\n",
    "m3x3x3 = [[[1,2,3],[4,5,6],[7,8,9]],\n",
    "          [[1,2,9],[16,25,36],[49,64,81]],\n",
    "         [[1,8,27],[64,125,216],[343,512,729]]]\n",
    "m3x3x3_tensor = torch.Tensor(m3x3x3)\n",
    "print(m3x3x3_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "print(m3x3x3_tensor[0]) ##index the 1st element "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Operation with Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([2,2,2])\n",
    "y = torch.Tensor([2,3,4])\n",
    "\n",
    "#matrix multiplication \n",
    "v = torch.matmul(x,y)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3251, -0.7990,  0.6292],\n",
      "        [-1.2097, -2.1362, -0.1212]])\n",
      "tensor([[-0.1443,  0.9969,  0.5697],\n",
      "        [-0.4930,  0.3155, -0.2275]])\n"
     ]
    }
   ],
   "source": [
    "##concatenation \n",
    "x_1 = torch.randn(2,3)\n",
    "print(x_1)\n",
    "y_1 = torch.randn(2,3)\n",
    "print(y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3251, -0.7990,  0.6292, -0.1443,  0.9969,  0.5697],\n",
      "        [-1.2097, -2.1362, -0.1212, -0.4930,  0.3155, -0.2275]])\n"
     ]
    }
   ],
   "source": [
    "z1 = torch.cat((x_1,y_1),1)  #concat along column axis\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64000,3,28,28) #64K images, 3 rgb channels, image dims = 28x28\n",
    "##how many batches can we construct out of these images\n",
    "x_reshaped = x.view(32,-1,3,28,28)  #batch_size=32, pytorch will automaticcally infer the dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2000, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(x_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph and Automatic Differentiation \n",
    "\n",
    "A computation graph is a speficication of what parameters with which connections are involveed int eh computation to give the output. Unlike tensorflow, pytorch's computational graph can be changed during run-time, is more flexible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "#variables wrap tensor objects\n",
    "x = autograd.Variable(torch.Tensor([1,2,3]), requires_grad=True) #takes arguments:  dataset and partial differentiation \n",
    "##access the data with 'data' attribute\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "y = autograd.Variable(torch.Tensor([4,5,6]),requires_grad=True)\n",
    "z= x+y\n",
    "print(z.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ThAddBackward object at 0x00000000069A5128>\n"
     ]
    }
   ],
   "source": [
    "##we can also see how z was created using grad_fn (i.e. function that created z )\n",
    "operation = z.grad_fn   #z was created by element-wiseaddition (Addbackward) operation \n",
    "print(operation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x0000000006B74FD0>\n"
     ]
    }
   ],
   "source": [
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)  #s was the sum of z elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so pytorch can track how every variable was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.backward(retain_graph=True) #backward will start backprop from this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([2., 2., 2.])\n",
      "tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)  ##partial differentiation wrt to each element in x \n",
    "print(y.grad)\n",
    "##each time backward is call the gradient will accumulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([3., 3., 3.])\n",
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "s.backward(retain_graph=True) #backward will start backprop from this point\n",
    "print(x)\n",
    "print(x.grad)  ##partial differentiation wrt to each element in x \n",
    "print(y.grad)\n",
    "##each time backward is call the gradient will accumulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ThAddBackward object at 0x0000000006B74B00>\n"
     ]
    }
   ],
   "source": [
    "var_x = autograd.Variable(x,requires_grad=True)\n",
    "var_y = autograd.Variable(y,requires_grad=True)\n",
    "#since var_z is addtion of two tensors thats enough to compute gradients\n",
    "var_z = var_x+var_y\n",
    "print(var_z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "new_var_z = var_z.data ##only data is passed \n",
    "print(new_var_z.grad_fn)b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA \n",
    "\n",
    "Check wether GPU acceleration is available cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    ##create a LongTensor and transfer it GPU\n",
    "    a = torch.LongTensor(10).fill(3).cuda()\n",
    "    print(a)\n",
    "    b = a.cpu()  #transfers it back to cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model in Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from torch.autograd import Variable  ##main building block of pyutorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "x = [i for i in range(20)]\n",
    "x_train=np.asarray(x, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1,1)  #flatten\n",
    "print(x)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [(5*i +2) for i in x]\n",
    "y_train = np.array(y,dtype=np.float32)\n",
    "y_train=y_train.reshape(-1,1)\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Class\n",
    "Every model in pytorch has to be created in a class (i.e. the blueprint or template )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressor(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegressor(nn.Module):  #we're inheriting from the nn module of pytorch \n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim,output_dim)\n",
    "    \n",
    "    def forward(self,x):  #x is the data \n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "input_dim = 1  #one column vector with 20 elements\n",
    "output_dim = 1\n",
    "\n",
    "model= LinearRegressor(input_dim, output_dim)\n",
    "model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss andn Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1, loss3776.293212890625\n",
      "epoch2, loss2133.56787109375\n",
      "epoch3, loss1205.7474365234375\n",
      "epoch4, loss681.7088623046875\n",
      "epoch5, loss385.72833251953125\n",
      "epoch6, loss218.55621337890625\n",
      "epoch7, loss124.13571166992188\n",
      "epoch8, loss70.805908203125\n",
      "epoch9, loss40.684104919433594\n",
      "epoch10, loss23.67044448852539\n",
      "epoch11, loss14.06031322479248\n",
      "epoch12, loss8.63171672821045\n",
      "epoch13, loss5.5648698806762695\n",
      "epoch14, loss3.831963539123535\n",
      "epoch15, loss2.852477788925171\n",
      "epoch16, loss2.298516273498535\n",
      "epoch17, loss1.9849095344543457\n",
      "epoch18, loss1.8070428371429443\n",
      "epoch19, loss1.7058515548706055\n",
      "epoch20, loss1.6479631662368774\n",
      "epoch21, loss1.6145336627960205\n",
      "epoch22, loss1.5949229001998901\n",
      "epoch23, loss1.5831143856048584\n",
      "epoch24, loss1.5757156610488892\n",
      "epoch25, loss1.5708060264587402\n",
      "epoch26, loss1.5673060417175293\n",
      "epoch27, loss1.5646004676818848\n",
      "epoch28, loss1.5623441934585571\n",
      "epoch29, loss1.5603443384170532\n",
      "epoch30, loss1.55849027633667\n",
      "epoch31, loss1.5567162036895752\n",
      "epoch32, loss1.5549908876419067\n",
      "epoch33, loss1.5532931089401245\n",
      "epoch34, loss1.5516122579574585\n",
      "epoch35, loss1.5499387979507446\n",
      "epoch36, loss1.5482743978500366\n",
      "epoch37, loss1.546613097190857\n",
      "epoch38, loss1.5449579954147339\n",
      "epoch39, loss1.5433019399642944\n",
      "epoch40, loss1.541651964187622\n",
      "epoch41, loss1.5400002002716064\n",
      "epoch42, loss1.5383529663085938\n",
      "epoch43, loss1.5367074012756348\n",
      "epoch44, loss1.5350593328475952\n",
      "epoch45, loss1.53341805934906\n",
      "epoch46, loss1.531774878501892\n",
      "epoch47, loss1.5301382541656494\n",
      "epoch48, loss1.5284991264343262\n",
      "epoch49, loss1.5268628597259521\n",
      "epoch50, loss1.5252275466918945\n",
      "epoch51, loss1.5235953330993652\n",
      "epoch52, loss1.5219656229019165\n",
      "epoch53, loss1.520337700843811\n",
      "epoch54, loss1.518709659576416\n",
      "epoch55, loss1.5170828104019165\n",
      "epoch56, loss1.515460729598999\n",
      "epoch57, loss1.5138369798660278\n",
      "epoch58, loss1.512218952178955\n",
      "epoch59, loss1.5105997323989868\n",
      "epoch60, loss1.508981466293335\n",
      "epoch61, loss1.507366418838501\n",
      "epoch62, loss1.5057528018951416\n",
      "epoch63, loss1.5041420459747314\n",
      "epoch64, loss1.502533197402954\n",
      "epoch65, loss1.5009247064590454\n",
      "epoch66, loss1.499320387840271\n",
      "epoch67, loss1.4977147579193115\n",
      "epoch68, loss1.4961124658584595\n",
      "epoch69, loss1.4945094585418701\n",
      "epoch70, loss1.4929091930389404\n",
      "epoch71, loss1.4913142919540405\n",
      "epoch72, loss1.4897170066833496\n",
      "epoch73, loss1.488122820854187\n",
      "epoch74, loss1.4865305423736572\n",
      "epoch75, loss1.4849364757537842\n",
      "epoch76, loss1.4833475351333618\n",
      "epoch77, loss1.48176109790802\n",
      "epoch78, loss1.4801772832870483\n",
      "epoch79, loss1.478590965270996\n",
      "epoch80, loss1.4770070314407349\n",
      "epoch81, loss1.4754282236099243\n",
      "epoch82, loss1.4738489389419556\n",
      "epoch83, loss1.4722709655761719\n",
      "epoch84, loss1.4706944227218628\n",
      "epoch85, loss1.469119906425476\n",
      "epoch86, loss1.4675486087799072\n",
      "epoch87, loss1.465977668762207\n",
      "epoch88, loss1.4644101858139038\n",
      "epoch89, loss1.4628404378890991\n",
      "epoch90, loss1.461276650428772\n",
      "epoch91, loss1.4597111940383911\n",
      "epoch92, loss1.45815110206604\n",
      "epoch93, loss1.4565887451171875\n",
      "epoch94, loss1.4550303220748901\n",
      "epoch95, loss1.4534729719161987\n",
      "epoch96, loss1.4519163370132446\n",
      "epoch97, loss1.4503633975982666\n",
      "epoch98, loss1.4488115310668945\n",
      "epoch99, loss1.4472603797912598\n",
      "epoch100, loss1.4457100629806519\n",
      "epoch101, loss1.4441646337509155\n",
      "epoch102, loss1.442619800567627\n",
      "epoch103, loss1.4410746097564697\n",
      "epoch104, loss1.439530611038208\n",
      "epoch105, loss1.4379901885986328\n",
      "epoch106, loss1.4364533424377441\n",
      "epoch107, loss1.434916377067566\n",
      "epoch108, loss1.4333802461624146\n",
      "epoch109, loss1.431847095489502\n",
      "epoch110, loss1.4303110837936401\n",
      "epoch111, loss1.4287818670272827\n",
      "epoch112, loss1.4272538423538208\n",
      "epoch113, loss1.4257245063781738\n",
      "epoch114, loss1.4242000579833984\n",
      "epoch115, loss1.4226747751235962\n",
      "epoch116, loss1.4211506843566895\n",
      "epoch117, loss1.4196324348449707\n",
      "epoch118, loss1.418113112449646\n",
      "epoch119, loss1.4165947437286377\n",
      "epoch120, loss1.41507887840271\n",
      "epoch121, loss1.4135613441467285\n",
      "epoch122, loss1.4120500087738037\n",
      "epoch123, loss1.410538911819458\n",
      "epoch124, loss1.4090279340744019\n",
      "epoch125, loss1.4075212478637695\n",
      "epoch126, loss1.4060128927230835\n",
      "epoch127, loss1.4045101404190063\n",
      "epoch128, loss1.4030061960220337\n",
      "epoch129, loss1.401504635810852\n",
      "epoch130, loss1.4000046253204346\n",
      "epoch131, loss1.3985060453414917\n",
      "epoch132, loss1.3970097303390503\n",
      "epoch133, loss1.3955141305923462\n",
      "epoch134, loss1.39402174949646\n",
      "epoch135, loss1.3925275802612305\n",
      "epoch136, loss1.3910380601882935\n",
      "epoch137, loss1.3895481824874878\n",
      "epoch138, loss1.3880629539489746\n",
      "epoch139, loss1.3865772485733032\n",
      "epoch140, loss1.3850915431976318\n",
      "epoch141, loss1.383608341217041\n",
      "epoch142, loss1.382128357887268\n",
      "epoch143, loss1.3806488513946533\n",
      "epoch144, loss1.3791712522506714\n",
      "epoch145, loss1.377695918083191\n",
      "epoch146, loss1.3762224912643433\n",
      "epoch147, loss1.374747633934021\n",
      "epoch148, loss1.3732775449752808\n",
      "epoch149, loss1.3718065023422241\n",
      "epoch150, loss1.3703398704528809\n",
      "epoch151, loss1.368872880935669\n",
      "epoch152, loss1.367406964302063\n",
      "epoch153, loss1.3659424781799316\n",
      "epoch154, loss1.3644806146621704\n",
      "epoch155, loss1.3630201816558838\n",
      "epoch156, loss1.3615623712539673\n",
      "epoch157, loss1.3601059913635254\n",
      "epoch158, loss1.3586505651474\n",
      "epoch159, loss1.3571943044662476\n",
      "epoch160, loss1.3557430505752563\n",
      "epoch161, loss1.3542910814285278\n",
      "epoch162, loss1.3528441190719604\n",
      "epoch163, loss1.3513951301574707\n",
      "epoch164, loss1.349948763847351\n",
      "epoch165, loss1.3485028743743896\n",
      "epoch166, loss1.3470591306686401\n",
      "epoch167, loss1.3456183671951294\n",
      "epoch168, loss1.3441787958145142\n",
      "epoch169, loss1.3427399396896362\n",
      "epoch170, loss1.3413045406341553\n",
      "epoch171, loss1.3398666381835938\n",
      "epoch172, loss1.338434100151062\n",
      "epoch173, loss1.3370009660720825\n",
      "epoch174, loss1.3355686664581299\n",
      "epoch175, loss1.334141731262207\n",
      "epoch176, loss1.332712173461914\n",
      "epoch177, loss1.331286072731018\n",
      "epoch178, loss1.3298614025115967\n",
      "epoch179, loss1.3284380435943604\n",
      "epoch180, loss1.3270164728164673\n",
      "epoch181, loss1.3255976438522339\n",
      "epoch182, loss1.3241771459579468\n",
      "epoch183, loss1.3227604627609253\n",
      "epoch184, loss1.3213430643081665\n",
      "epoch185, loss1.3199301958084106\n",
      "epoch186, loss1.3185175657272339\n",
      "epoch187, loss1.317104697227478\n",
      "epoch188, loss1.315698266029358\n",
      "epoch189, loss1.3142895698547363\n",
      "epoch190, loss1.312882900238037\n",
      "epoch191, loss1.3114778995513916\n",
      "epoch192, loss1.3100706338882446\n",
      "epoch193, loss1.3086700439453125\n",
      "epoch194, loss1.307269811630249\n",
      "epoch195, loss1.3058689832687378\n",
      "epoch196, loss1.3044729232788086\n",
      "epoch197, loss1.3030750751495361\n",
      "epoch198, loss1.301682710647583\n",
      "epoch199, loss1.3002891540527344\n",
      "epoch200, loss1.2988959550857544\n",
      "epoch201, loss1.2975064516067505\n",
      "epoch202, loss1.296116590499878\n",
      "epoch203, loss1.2947301864624023\n",
      "epoch204, loss1.2933452129364014\n",
      "epoch205, loss1.2919609546661377\n",
      "epoch206, loss1.2905797958374023\n",
      "epoch207, loss1.289196252822876\n",
      "epoch208, loss1.2878179550170898\n",
      "epoch209, loss1.2864388227462769\n",
      "epoch210, loss1.2850639820098877\n",
      "epoch211, loss1.2836863994598389\n",
      "epoch212, loss1.2823131084442139\n",
      "epoch213, loss1.2809406518936157\n",
      "epoch214, loss1.2795692682266235\n",
      "epoch215, loss1.278199553489685\n",
      "epoch216, loss1.276831865310669\n",
      "epoch217, loss1.2754656076431274\n",
      "epoch218, loss1.2741020917892456\n",
      "epoch219, loss1.272736668586731\n",
      "epoch220, loss1.2713756561279297\n",
      "epoch221, loss1.270013451576233\n",
      "epoch222, loss1.2686563730239868\n",
      "epoch223, loss1.2672966718673706\n",
      "epoch224, loss1.2659403085708618\n",
      "epoch225, loss1.264585256576538\n",
      "epoch226, loss1.2632311582565308\n",
      "epoch227, loss1.261878490447998\n",
      "epoch228, loss1.2605293989181519\n",
      "epoch229, loss1.2591804265975952\n",
      "epoch230, loss1.2578339576721191\n",
      "epoch231, loss1.2564855813980103\n",
      "epoch232, loss1.255141258239746\n",
      "epoch233, loss1.2537964582443237\n",
      "epoch234, loss1.2524561882019043\n",
      "epoch235, loss1.2511157989501953\n",
      "epoch236, loss1.2497748136520386\n",
      "epoch237, loss1.2484405040740967\n",
      "epoch238, loss1.2471038103103638\n",
      "epoch239, loss1.2457693815231323\n",
      "epoch240, loss1.2444366216659546\n",
      "epoch241, loss1.243101716041565\n",
      "epoch242, loss1.2417722940444946\n",
      "epoch243, loss1.2404435873031616\n",
      "epoch244, loss1.2391141653060913\n",
      "epoch245, loss1.2377891540527344\n",
      "epoch246, loss1.2364630699157715\n",
      "epoch247, loss1.235142469406128\n",
      "epoch248, loss1.233819603919983\n",
      "epoch249, loss1.2324984073638916\n",
      "epoch250, loss1.231178879737854\n",
      "epoch251, loss1.2298611402511597\n",
      "epoch252, loss1.2285443544387817\n",
      "epoch253, loss1.2272309064865112\n",
      "epoch254, loss1.2259174585342407\n",
      "epoch255, loss1.2246066331863403\n",
      "epoch256, loss1.223293662071228\n",
      "epoch257, loss1.221985101699829\n",
      "epoch258, loss1.2206761837005615\n",
      "epoch259, loss1.2193713188171387\n",
      "epoch260, loss1.2180657386779785\n",
      "epoch261, loss1.2167614698410034\n",
      "epoch262, loss1.2154581546783447\n",
      "epoch263, loss1.2141607999801636\n",
      "epoch264, loss1.2128608226776123\n",
      "epoch265, loss1.2115603685379028\n",
      "epoch266, loss1.210263729095459\n",
      "epoch267, loss1.2089699506759644\n",
      "epoch268, loss1.2076762914657593\n",
      "epoch269, loss1.2063816785812378\n",
      "epoch270, loss1.2050915956497192\n",
      "epoch271, loss1.2038010358810425\n",
      "epoch272, loss1.2025153636932373\n",
      "epoch273, loss1.201225996017456\n",
      "epoch274, loss1.1999412775039673\n",
      "epoch275, loss1.1986565589904785\n",
      "epoch276, loss1.197373867034912\n",
      "epoch277, loss1.1960917711257935\n",
      "epoch278, loss1.1948121786117554\n",
      "epoch279, loss1.193533182144165\n",
      "epoch280, loss1.1922571659088135\n",
      "epoch281, loss1.190978765487671\n",
      "epoch282, loss1.1897053718566895\n",
      "epoch283, loss1.1884304285049438\n",
      "epoch284, loss1.1871610879898071\n",
      "epoch285, loss1.1858891248703003\n",
      "epoch286, loss1.1846188306808472\n",
      "epoch287, loss1.1833508014678955\n",
      "epoch288, loss1.1820871829986572\n",
      "epoch289, loss1.1808220148086548\n",
      "epoch290, loss1.1795551776885986\n",
      "epoch291, loss1.1782927513122559\n",
      "epoch292, loss1.1770330667495728\n",
      "epoch293, loss1.1757736206054688\n",
      "epoch294, loss1.1745129823684692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch295, loss1.1732573509216309\n",
      "epoch296, loss1.1720002889633179\n",
      "epoch297, loss1.170747995376587\n",
      "epoch298, loss1.1694941520690918\n",
      "epoch299, loss1.1682419776916504\n",
      "epoch300, loss1.1669942140579224\n",
      "epoch301, loss1.1657447814941406\n",
      "epoch302, loss1.1644971370697021\n",
      "epoch303, loss1.1632481813430786\n",
      "epoch304, loss1.1620036363601685\n",
      "epoch305, loss1.1607613563537598\n",
      "epoch306, loss1.1595184803009033\n",
      "epoch307, loss1.1582765579223633\n",
      "epoch308, loss1.157037377357483\n",
      "epoch309, loss1.1557985544204712\n",
      "epoch310, loss1.154563307762146\n",
      "epoch311, loss1.1533268690109253\n",
      "epoch312, loss1.1520922183990479\n",
      "epoch313, loss1.1508580446243286\n",
      "epoch314, loss1.1496262550354004\n",
      "epoch315, loss1.1483954191207886\n",
      "epoch316, loss1.1471666097640991\n",
      "epoch317, loss1.1459401845932007\n",
      "epoch318, loss1.1447142362594604\n",
      "epoch319, loss1.143490195274353\n",
      "epoch320, loss1.142264485359192\n",
      "epoch321, loss1.141042947769165\n",
      "epoch322, loss1.1398208141326904\n",
      "epoch323, loss1.138599157333374\n",
      "epoch324, loss1.1373831033706665\n",
      "epoch325, loss1.136164903640747\n",
      "epoch326, loss1.1349492073059082\n",
      "epoch327, loss1.1337347030639648\n",
      "epoch328, loss1.1325209140777588\n",
      "epoch329, loss1.1313087940216064\n",
      "epoch330, loss1.1300979852676392\n",
      "epoch331, loss1.1288896799087524\n",
      "epoch332, loss1.127679705619812\n",
      "epoch333, loss1.1264735460281372\n",
      "epoch334, loss1.1252660751342773\n",
      "epoch335, loss1.1240633726119995\n",
      "epoch336, loss1.122860074043274\n",
      "epoch337, loss1.1216565370559692\n",
      "epoch338, loss1.1204593181610107\n",
      "epoch339, loss1.1192599534988403\n",
      "epoch340, loss1.1180611848831177\n",
      "epoch341, loss1.1168644428253174\n",
      "epoch342, loss1.115670084953308\n",
      "epoch343, loss1.1144734621047974\n",
      "epoch344, loss1.1132819652557373\n",
      "epoch345, loss1.1120903491973877\n",
      "epoch346, loss1.1108989715576172\n",
      "epoch347, loss1.1097114086151123\n",
      "epoch348, loss1.1085220575332642\n",
      "epoch349, loss1.107337236404419\n",
      "epoch350, loss1.1061519384384155\n",
      "epoch351, loss1.104966402053833\n",
      "epoch352, loss1.1037834882736206\n",
      "epoch353, loss1.1026053428649902\n",
      "epoch354, loss1.1014248132705688\n",
      "epoch355, loss1.1002435684204102\n",
      "epoch356, loss1.0990670919418335\n",
      "epoch357, loss1.097890853881836\n",
      "epoch358, loss1.0967168807983398\n",
      "epoch359, loss1.09554123878479\n",
      "epoch360, loss1.0943701267242432\n",
      "epoch361, loss1.0931971073150635\n",
      "epoch362, loss1.092029094696045\n",
      "epoch363, loss1.0908595323562622\n",
      "epoch364, loss1.0896915197372437\n",
      "epoch365, loss1.0885246992111206\n",
      "epoch366, loss1.0873593091964722\n",
      "epoch367, loss1.0861952304840088\n",
      "epoch368, loss1.0850332975387573\n",
      "epoch369, loss1.0838727951049805\n",
      "epoch370, loss1.0827125310897827\n",
      "epoch371, loss1.0815551280975342\n",
      "epoch372, loss1.0803951025009155\n",
      "epoch373, loss1.0792404413223267\n",
      "epoch374, loss1.0780837535858154\n",
      "epoch375, loss1.076932430267334\n",
      "epoch376, loss1.0757789611816406\n",
      "epoch377, loss1.0746269226074219\n",
      "epoch378, loss1.0734754800796509\n",
      "epoch379, loss1.0723294019699097\n",
      "epoch380, loss1.0711820125579834\n",
      "epoch381, loss1.0700324773788452\n",
      "epoch382, loss1.06888747215271\n",
      "epoch383, loss1.0677446126937866\n",
      "epoch384, loss1.0666018724441528\n",
      "epoch385, loss1.0654616355895996\n",
      "epoch386, loss1.0643199682235718\n",
      "epoch387, loss1.0631824731826782\n",
      "epoch388, loss1.062043309211731\n",
      "epoch389, loss1.0609052181243896\n",
      "epoch390, loss1.0597715377807617\n",
      "epoch391, loss1.0586369037628174\n",
      "epoch392, loss1.057503581047058\n",
      "epoch393, loss1.0563716888427734\n",
      "epoch394, loss1.0552407503128052\n",
      "epoch395, loss1.0541112422943115\n",
      "epoch396, loss1.0529848337173462\n",
      "epoch397, loss1.0518548488616943\n",
      "epoch398, loss1.0507303476333618\n",
      "epoch399, loss1.049607276916504\n",
      "epoch400, loss1.0484817028045654\n",
      "epoch401, loss1.0473618507385254\n",
      "epoch402, loss1.0462400913238525\n",
      "epoch403, loss1.045118808746338\n",
      "epoch404, loss1.0440024137496948\n",
      "epoch405, loss1.042884111404419\n",
      "epoch406, loss1.0417674779891968\n",
      "epoch407, loss1.0406523942947388\n",
      "epoch408, loss1.0395393371582031\n",
      "epoch409, loss1.0384271144866943\n",
      "epoch410, loss1.0373159646987915\n",
      "epoch411, loss1.0362048149108887\n",
      "epoch412, loss1.0350956916809082\n",
      "epoch413, loss1.0339866876602173\n",
      "epoch414, loss1.0328807830810547\n",
      "epoch415, loss1.0317744016647339\n",
      "epoch416, loss1.030672550201416\n",
      "epoch417, loss1.0295681953430176\n",
      "epoch418, loss1.0284664630889893\n",
      "epoch419, loss1.0273648500442505\n",
      "epoch420, loss1.026265263557434\n",
      "epoch421, loss1.025166392326355\n",
      "epoch422, loss1.0240706205368042\n",
      "epoch423, loss1.0229747295379639\n",
      "epoch424, loss1.0218794345855713\n",
      "epoch425, loss1.020787000656128\n",
      "epoch426, loss1.0196927785873413\n",
      "epoch427, loss1.018602967262268\n",
      "epoch428, loss1.017511248588562\n",
      "epoch429, loss1.016424298286438\n",
      "epoch430, loss1.0153342485427856\n",
      "epoch431, loss1.014248013496399\n",
      "epoch432, loss1.0131614208221436\n",
      "epoch433, loss1.0120770931243896\n",
      "epoch434, loss1.010993242263794\n",
      "epoch435, loss1.0099116563796997\n",
      "epoch436, loss1.0088310241699219\n",
      "epoch437, loss1.0077521800994873\n",
      "epoch438, loss1.0066736936569214\n",
      "epoch439, loss1.005597710609436\n",
      "epoch440, loss1.0045199394226074\n",
      "epoch441, loss1.003446340560913\n",
      "epoch442, loss1.002371072769165\n",
      "epoch443, loss1.0012962818145752\n",
      "epoch444, loss1.000227451324463\n",
      "epoch445, loss0.9991554617881775\n",
      "epoch446, loss0.9980860352516174\n",
      "epoch447, loss0.9970178604125977\n",
      "epoch448, loss0.9959510564804077\n",
      "epoch449, loss0.9948850274085999\n",
      "epoch450, loss0.9938204288482666\n",
      "epoch451, loss0.9927572011947632\n",
      "epoch452, loss0.991695761680603\n",
      "epoch453, loss0.9906324148178101\n",
      "epoch454, loss0.9895728230476379\n",
      "epoch455, loss0.9885121583938599\n",
      "epoch456, loss0.9874555468559265\n",
      "epoch457, loss0.986398458480835\n",
      "epoch458, loss0.9853413105010986\n",
      "epoch459, loss0.9842894673347473\n",
      "epoch460, loss0.9832351207733154\n",
      "epoch461, loss0.9821820855140686\n",
      "epoch462, loss0.9811311960220337\n",
      "epoch463, loss0.9800813794136047\n",
      "epoch464, loss0.979033350944519\n",
      "epoch465, loss0.9779829382896423\n",
      "epoch466, loss0.9769372344017029\n",
      "epoch467, loss0.9758920073509216\n",
      "epoch468, loss0.9748459458351135\n",
      "epoch469, loss0.973804235458374\n",
      "epoch470, loss0.9727607369422913\n",
      "epoch471, loss0.9717219471931458\n",
      "epoch472, loss0.9706810116767883\n",
      "epoch473, loss0.9696418642997742\n",
      "epoch474, loss0.9686025381088257\n",
      "epoch475, loss0.9675657153129578\n",
      "epoch476, loss0.9665306806564331\n",
      "epoch477, loss0.9654961228370667\n",
      "epoch478, loss0.9644626379013062\n",
      "epoch479, loss0.963431715965271\n",
      "epoch480, loss0.9624009728431702\n",
      "epoch481, loss0.9613693952560425\n",
      "epoch482, loss0.9603413939476013\n",
      "epoch483, loss0.95931476354599\n",
      "epoch484, loss0.9582867622375488\n",
      "epoch485, loss0.9572601914405823\n",
      "epoch486, loss0.9562384486198425\n",
      "epoch487, loss0.9552133679389954\n",
      "epoch488, loss0.954190731048584\n",
      "epoch489, loss0.953169584274292\n",
      "epoch490, loss0.9521499872207642\n",
      "epoch491, loss0.9511306285858154\n",
      "epoch492, loss0.9501121640205383\n",
      "epoch493, loss0.9490963816642761\n",
      "epoch494, loss0.9480787515640259\n",
      "epoch495, loss0.9470650553703308\n",
      "epoch496, loss0.9460528492927551\n",
      "epoch497, loss0.9450385570526123\n",
      "epoch498, loss0.944028377532959\n",
      "epoch499, loss0.9430174827575684\n",
      "epoch500, loss0.9420066475868225\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    epoch+=1\n",
    "    ##convert inputs and outputs to torch variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    real_outputs = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #forward pass - compute the output\n",
    "    pred_outputs = model(inputs)\n",
    "    \n",
    "    ## compute loss:  how close are the predictions to true values \n",
    "    loss = loss_function(pred_outputs,real_outputs)\n",
    "    \n",
    "    ##backward pass -  compute the gradients\n",
    "    loss.backward()\n",
    "    ##update the parameters in a way that [hopefully] minimise the loss\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('epoch{}, loss{}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Autoencoder LSTM Anamoly Detector in keras\n",
    "\n",
    "### this lesson may be done on the ibmWatson cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ibmiotf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-b2955c404a71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mibmiotf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplication\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mqueue\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQueue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ibmiotf'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import sklearn\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ibmiotf.application\n",
    "from queue import Queue\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ibmiotf\n",
      "  Downloading https://files.pythonhosted.org/packages/78/05/029ca6f78b788a3c55157fd11bb63922d002d75df982ffb8243f450a750e/ibmiotf-0.4.0.tar.gz (71kB)\n",
      "Collecting iso8601>=0.1.12 (from ibmiotf)\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/57/7162609dab394d38bbc7077b7ba0a6f10fb09d8b7701ea56fa1edc0c4345/iso8601-0.1.12-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\program files\\python36\\lib\\site-packages (from ibmiotf) (2018.3)\n",
      "Collecting paho-mqtt>=1.3.1 (from ibmiotf)\n",
      "  Downloading https://files.pythonhosted.org/packages/25/63/db25e62979c2a716a74950c9ed658dce431b5cb01fde29eb6cba9489a904/paho-mqtt-1.4.0.tar.gz (88kB)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\program files\\python36\\lib\\site-packages (from ibmiotf) (2.18.4)\n",
      "Collecting requests_toolbelt>=0.8.0 (from ibmiotf)\n",
      "  Downloading https://files.pythonhosted.org/packages/97/8a/d710f792d6f6ecc089c5e55b66e66c3f2f35516a1ede5a8f54c13350ffb0/requests_toolbelt-0.8.0-py2.py3-none-any.whl (54kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python36\\lib\\site-packages (from requests>=2.18.4->ibmiotf) (2018.1.18)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\program files\\python36\\lib\\site-packages (from requests>=2.18.4->ibmiotf) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\program files\\python36\\lib\\site-packages (from requests>=2.18.4->ibmiotf) (1.22)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\program files\\python36\\lib\\site-packages (from requests>=2.18.4->ibmiotf) (2.6)\n",
      "Building wheels for collected packages: ibmiotf, paho-mqtt\n",
      "  Running setup.py bdist_wheel for ibmiotf: started\n",
      "  Running setup.py bdist_wheel for ibmiotf: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\My LENOVO\\AppData\\Local\\pip\\Cache\\wheels\\ea\\fe\\e0\\855ec0be95b6ae2d7824e99ff5a7a0bca357b8d5807197c894\n",
      "  Running setup.py bdist_wheel for paho-mqtt: started\n",
      "  Running setup.py bdist_wheel for paho-mqtt: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\My LENOVO\\AppData\\Local\\pip\\Cache\\wheels\\82\\e5\\de\\d90d0f397648a1b58ffeea1b5742ac8c77f71fd43b550fa5a5\n",
      "Successfully built ibmiotf paho-mqtt\n",
      "Installing collected packages: iso8601, paho-mqtt, requests-toolbelt, ibmiotf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\program files\\\\python36\\\\Lib\\\\site-packages\\\\iso8601'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install ibmiotf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'--user' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/lorenzattractor/watsoniotp.healthy.phase_aligned.pickle\n",
    "!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/lorenzattractor/watsoniotp.broken.phase_aligned.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
